import os
import sys
import random
import atexit
import signal
from typing import List, Optional
import numpy as np
import torch
import torch.distributed as dist
from datetime import timedelta
from utils import line, printM, printS, printE, printW, MASTER_RANK


def set_seed(seed: int, use_deterministic: bool = True) -> None:
    """
    ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
    
    Args:
        seed: ëœë¤ ì‹œë“œ ê°’
        use_deterministic: Trueë©´ ì™„ì „ ê²°ì •ë¡ ì (ëŠë¦¼), Falseë©´ ë¹ ë¥´ì§€ë§Œ ì•½ê°„ì˜ ë¹„ê²°ì •ì„±
    """
    # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    # Python/NumPy/PyTorch ì‹œë“œ
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # ë©€í‹° GPU
    
    if use_deterministic:
        # ì™„ì „í•œ ì¬í˜„ì„± (ëŠë¦¼)
        os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.use_deterministic_algorithms(True, warn_only=True)
    else:
        # ë¹ ë¥¸ í•™ìŠµ (ì•½ê°„ì˜ ë¹„ê²°ì •ì„±)
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True


class DDPManager:
    """ë¶„ì‚° í•™ìŠµ(DDP) ë° ë””ë°”ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, gpus: List[int]):
        """
        Args:
            gpus: ì‚¬ìš©í•  GPU ID ë¦¬ìŠ¤íŠ¸. [-1]ì´ë©´ CPU ëª¨ë“œ
        """
        self.cpu = gpus[0] == -1
        self.num_gpus = len(gpus)
        self.target_gpu_ids = gpus
        self.rank = 0
        self.local_rank = 0
        self.world_size = 1
        self.device: Optional[torch.device] = None
        self.is_master = True
        self.ddp_initialized = False
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        atexit.register(self.cleanup)
        
        self.setup_device()
    
    @line
    def setup_device(self) -> None:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • (CPU/Single GPU/DDP)"""
        printM(" [GPU Setting]\n", 'blue')
        
        if self.cpu or not torch.cuda.is_available():
            if not torch.cuda.is_available():
                printW('CUDA is not available')
            self.set_cpu()
        elif self.num_gpus == 1:
            self.set_cuda()
        else:
            # DDP í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            if self._is_ddp_environment():
                self.set_ddp()
            else:
                printW(f"{self.num_gpus}ê°œì˜ GPUë¥¼ ìš”ì²­í–ˆìœ¼ë‚˜ DDP í™˜ê²½ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                printM(" ì‹±ê¸€ GPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.\n", 'blue')
                self.set_cuda()
    
    @staticmethod
    def _is_ddp_environment() -> bool:
        """DDP í™˜ê²½ì¸ì§€ í™•ì¸"""
        return 'RANK' in os.environ and 'WORLD_SIZE' in os.environ and 'LOCAL_RANK' in os.environ
    
    def optimize_cpu_threads(self, num_gpus: int) -> None:
        """CPU ìŠ¤ë ˆë“œ ìµœì í™”"""
        cpu_count = os.cpu_count() or 1
        optimal_threads = max(1, cpu_count // num_gpus // 2)
        os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
        printS(f"OMP_NUM_THREADS set to {optimal_threads}")
    
    def init_ddp(self, device_id: int) -> None:
        """
        DDP ì´ˆê¸°í™”
        
        Args:
            device_id: í˜„ì¬ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš©í•  GPU ID
        """
        if dist.is_initialized():
            printW("DDP already initialized")
            return
        
        try:
            dist.init_process_group(
                backend='nccl',
                init_method='env://',
                timeout=timedelta(seconds=1800),
                device_id=torch.device(f'cuda:{device_id}')  # ğŸ”¥ ê²½ê³  í•´ê²°
            )
            self.ddp_initialized = True
        except Exception as e:
            raise RuntimeError(f"DDP ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def ddp_check(self, available_gpus):
        for gpu_id in self.target_gpu_ids:
            assert gpu_id < available_gpus, f"GPU {gpu_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    def set_ddp(self) -> None:
        """DDP ì„¤ì •"""
        # í™˜ê²½ë³€ìˆ˜ ê²€ì¦
        if not self._is_ddp_environment():
            raise RuntimeError(
                " DDP ëª¨ë“œëŠ” torchrunìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n"
                " ì˜ˆ: torchrun --nproc_per_node=2 train.py"
            )
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ rank ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        local_rank_idx = int(os.environ['LOCAL_RANK'])
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.local_rank = local_rank_idx
        
        # GPU ë§¤í•‘
        available_gpus = torch.cuda.device_count()
        self.ddp_check(available_gpus)
        
        if available_gpus == 0:
            raise RuntimeError("CUDA GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        # GPU ID ê²°ì •
        gpu_id = self.target_gpu_ids[local_rank_idx]
        torch.cuda.set_device(gpu_id)
        self.device = torch.device(f'cuda:{gpu_id}')
        
        # CPU ìŠ¤ë ˆë“œ ìµœì í™”
        self.optimize_cpu_threads(self.world_size)
        
        # DDP ì´ˆê¸°í™” (device_id ì „ë‹¬)
        self.init_ddp(device_id=gpu_id)
        
        # ë§ˆìŠ¤í„° í”„ë¡œì„¸ìŠ¤ ì—¬ë¶€
        self.is_master = self.rank == 0
        
        # ì •ë³´ ì¶œë ¥ (ë§ˆìŠ¤í„°ë§Œ)
        if self.is_master:
            printS(f"DDP Mode Activated")
            printS(f"World Size: {self.world_size}")
            printS(f"Backend: nccl")
            printS(f"Target GPUs: {self.target_gpu_ids}")
            printS(f"Available GPUs: {available_gpus}")
        
        printS(f"Rank {self.rank}: Using GPU {gpu_id} ({torch.cuda.get_device_name(gpu_id)})")
        
        # ë™ê¸°í™” (ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì—¬ê¸°ê¹Œì§€ ë„ë‹¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°)
        dist.barrier()
    
    def _signal_handler(self, signum: int, frame) -> None:
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
        printS(f"\n\n Received signal {signum}, cleaning up...")
        self.cleanup()
        sys.exit(0)
    
    def cleanup(self) -> None:
        """DDP ì •ë¦¬"""
        if self.ddp_initialized and dist.is_initialized():
            try:
                dist.barrier()  # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ë™ê¸°í™”
                dist.destroy_process_group()
                printS("DDP process group destroyed.")
            except Exception as e:
                printW(f"Error during DDP cleanup: {e}")
            finally:
                self.ddp_initialized = False
    
    def set_cuda(self) -> None:
        """Single GPU ëª¨ë“œ ì„¤ì •"""
        self.rank = 0
        self.world_size = 1
        self.local_rank = 0
        self.is_master = True
        
        # GPU ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        available_gpus = torch.cuda.device_count()
        if available_gpus == 0:
            printW("CUDA ì‚¬ìš© ê°€ëŠ¥í•œ GPUê°€ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            return self.set_cpu()
        
        # GPU ID ê²°ì •
        if not self.target_gpu_ids or self.target_gpu_ids[0] == -1:
            printW("target_gpu_idsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. GPU 0ë²ˆì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            gpu_id = 0
        else:
            gpu_id = self.target_gpu_ids[0]
        
        # GPU ìœ íš¨ì„± ê²€ì¦
        if gpu_id >= available_gpus:
            printW(f"GPU {gpu_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. GPU 0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            gpu_id = 0
        
        # CPU ìŠ¤ë ˆë“œ ìµœì í™”
        self.optimize_cpu_threads(1)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        torch.cuda.set_device(gpu_id)
        self.device = torch.device(f'cuda:{gpu_id}')
        
        printS(f"Using Single GPU: {gpu_id}")
        printS(f"GPU Name: {torch.cuda.get_device_name(gpu_id)}")
    
    def set_cpu(self) -> None:
        """CPU ëª¨ë“œ ì„¤ì •"""
        self.rank = 0
        self.world_size = 1
        self.local_rank = 0
        self.is_master = True
        self.device = torch.device('cpu')
        
        printS("Using CPU")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        self.cleanup()


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì‹œë“œ ì„¤ì •
    set_seed(42, use_deterministic=True)
    
    # DDP ë§¤ë‹ˆì € ì´ˆê¸°í™”
    # torchrun --nproc_per_node=3 train.py ë¡œ ì‹¤í–‰
    ddp_manager = DDPManager(gpus=[0, 1, 2])
    
    print(f"Device: {ddp_manager.device}")
    print(f"Rank: {ddp_manager.rank}")
    print(f"World Size: {ddp_manager.world_size}")
    print(f"Is Master: {ddp_manager.is_master}")