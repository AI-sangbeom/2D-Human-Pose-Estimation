{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e34c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "backbone = torch.hub.load(\"models/backbone/dinov3\", 'dinov3_convnext_small', source='local', weights='./checkpoints/dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth')\n",
    "# backbone.forward_features(torch.randn(1, 3, 224, 224))[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db96197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPooling(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7, sigma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # Create Gaussian kernel\n",
    "        ax = torch.arange(-self.padding, self.padding + 1, device=device).float()\n",
    "        xx, yy = torch.meshgrid(ax, ax)\n",
    "        kernel = torch.exp(-(xx**2 + yy**2) / (2 * self.sigma**2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        kernel = kernel.view(1, 1, self.kernel_size, self.kernel_size).repeat(channels, 1, 1, 1)\n",
    "\n",
    "        # Apply Gaussian pooling\n",
    "        x = F.conv2d(x, kernel, padding=self.padding, groups=channels)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b45ede77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Output shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "pool = GaussianPooling(kernel_size=5, sigma=2.0)\n",
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "output_tensor = pool(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class GatedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Multi-Head Attention from \"Gated Attention for Large Language Models: \n",
    "    Non-linearity, Sparsity, and Attention-Sink-Free\" (arXiv:2505.06708)\n",
    "    \n",
    "    핵심 아이디어:\n",
    "    - SDPA (Scaled Dot-Product Attention) 출력 후에 sigmoid gate를 적용\n",
    "    - 수식: Y' = Y ⊙ σ(XW_θ)\n",
    "    - head-specific한 gating으로 attention sink 현상 완화\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        gate_type: str = \"elementwise\",  # \"elementwise\" or \"headwise\"\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 모델의 hidden dimension\n",
    "            num_heads: attention head 수\n",
    "            dropout: dropout 비율\n",
    "            gate_type: \"elementwise\" (각 원소별) 또는 \"headwise\" (head별 단일 게이트)\n",
    "            bias: linear layer에 bias 사용 여부\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.gate_type = gate_type\n",
    "        \n",
    "        # Q, K, V projection layers\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=bias)\n",
    "        \n",
    "        # Gate projection (핵심 추가 부분)\n",
    "        # elementwise: 각 차원마다 독립적인 gate\n",
    "        # headwise: 각 head마다 하나의 gate 값\n",
    "        if gate_type == \"elementwise\":\n",
    "            self.W_gate = nn.Linear(d_model, d_model, bias=bias)\n",
    "        elif gate_type == \"headwise\":\n",
    "            self.W_gate = nn.Linear(d_model, num_heads, bias=bias)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown gate_type: {gate_type}\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "        return_gate_scores: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Attention mask (batch_size, seq_len, seq_len) or (batch_size, 1, seq_len, seq_len)\n",
    "            return_gate_scores: gate 값도 반환할지 여부\n",
    "            \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_model)\n",
    "            gate_scores (optional): gate 값들\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 1. Linear projections for Q, K, V\n",
    "        Q = self.W_q(x)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # 2. Split into multiple heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 3. Scaled Dot-Product Attention (SDPA)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)  # Add head dimension\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Attention output\n",
    "        Y = torch.matmul(attn_weights, V)\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 4. 핵심: Gating mechanism (논문의 G1 position)\n",
    "        # Compute gate scores: σ(XW_θ)\n",
    "        gate_logits = self.W_gate(x)  # (batch_size, seq_len, d_model or num_heads)\n",
    "        gate_scores = torch.sigmoid(gate_logits)\n",
    "        \n",
    "        if self.gate_type == \"elementwise\":\n",
    "            # Element-wise gating: 각 차원마다 독립적인 gate\n",
    "            gate_scores = gate_scores.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "            gate_scores = gate_scores.transpose(1, 2)\n",
    "            # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            \n",
    "            # Apply gate: Y' = Y ⊙ σ(XW_θ)\n",
    "            Y_gated = Y * gate_scores\n",
    "            \n",
    "        elif self.gate_type == \"headwise\":\n",
    "            # Head-wise gating: 각 head마다 하나의 gate 값\n",
    "            gate_scores = gate_scores.view(batch_size, seq_len, self.num_heads, 1)\n",
    "            gate_scores = gate_scores.transpose(1, 2)\n",
    "            # Shape: (batch_size, num_heads, seq_len, 1)\n",
    "            \n",
    "            # Apply gate\n",
    "            Y_gated = Y * gate_scores\n",
    "        \n",
    "        # 5. Concatenate heads\n",
    "        Y_gated = Y_gated.transpose(1, 2).contiguous()\n",
    "        # Shape: (batch_size, seq_len, num_heads, d_k)\n",
    "        Y_gated = Y_gated.view(batch_size, seq_len, d_model)\n",
    "        # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 6. Final output projection\n",
    "        output = self.W_o(Y_gated)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        if return_gate_scores:\n",
    "            return output, gate_scores\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea75fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MetaSpace with Keypoint Features - Example\n",
      "============================================================\n",
      "\n",
      "1. Initial forward pass (training mode):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (17) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 414\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. Initial forward pass (training mode):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    413\u001b[0m meta_space\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 414\u001b[0m fused_features \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, feats \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fused_features):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeats\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/hpe/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/hpe/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 369\u001b[0m, in \u001b[0;36mMetaSpace.forward\u001b[0;34m(self, feature_maps, keypoints, valid_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m kpt_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_keypoint_features(feature_map, keypoints)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# 2. Accumulate for meta space update (training only)\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkpt_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# 3. Fuse with meta features\u001b[39;00m\n\u001b[1;32m    372\u001b[0m fused \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuse_with_meta_features(level_idx, kpt_feats)\n",
      "Cell \u001b[0;32mIn[2], line 268\u001b[0m, in \u001b[0;36mMetaSpace.accumulate_features\u001b[0;34m(self, level_idx, kpt_features, valid_mask)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# Only accumulate valid keypoints\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     valid_mask \u001b[38;5;241m=\u001b[39m valid_mask\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# (B, N)\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     masked_features \u001b[38;5;241m=\u001b[39m \u001b[43mkpt_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalid_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, N, C)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     feature_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m masked_features\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (N, C)\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     feature_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m valid_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (N,)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (17) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, List\n",
    "import math\n",
    "\n",
    "\n",
    "class GaussianPooling(nn.Module):\n",
    "    \"\"\"Gaussian weighted pooling for local features\"\"\"\n",
    "    def __init__(self, kernel_size: int = 5, sigma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Create Gaussian kernel\n",
    "        kernel = self._create_gaussian_kernel(kernel_size, sigma)\n",
    "        self.register_buffer('kernel', kernel)\n",
    "        \n",
    "    def _create_gaussian_kernel(self, kernel_size: int, sigma: float) -> torch.Tensor:\n",
    "        \"\"\"Create 2D Gaussian kernel\"\"\"\n",
    "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.)\n",
    "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
    "        kernel = torch.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        return kernel.unsqueeze(0).unsqueeze(0)  # (1, 1, k, k)\n",
    "    \n",
    "    def forward(self, feature_map: torch.Tensor, keypoints: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract Gaussian-weighted features around keypoints\n",
    "        \n",
    "        Args:\n",
    "            feature_map: (C, H, W) feature map\n",
    "            keypoints: (N, 2) keypoint coordinates [x, y]\n",
    "            \n",
    "        Returns:\n",
    "            pooled_features: (N, C) features for each keypoint\n",
    "        \"\"\"\n",
    "        C, H, W = feature_map.shape\n",
    "        N = keypoints.shape[0]\n",
    "        half_k = self.kernel_size // 2\n",
    "        \n",
    "        pooled_features = []\n",
    "        \n",
    "        for kpt in keypoints:\n",
    "            x, y = kpt[0].long(), kpt[1].long()\n",
    "            \n",
    "            # Boundary check and padding\n",
    "            x = torch.clamp(x, half_k, W - half_k - 1)\n",
    "            y = torch.clamp(y, half_k, H - half_k - 1)\n",
    "            \n",
    "            # Extract local patch\n",
    "            patch = feature_map[:, \n",
    "                              y - half_k:y + half_k + 1,\n",
    "                              x - half_k:x + half_k + 1]  # (C, k, k)\n",
    "            \n",
    "            # Apply Gaussian weighting\n",
    "            if patch.shape[1:] == (self.kernel_size, self.kernel_size):\n",
    "                weighted = patch * self.kernel  # (C, k, k)\n",
    "                pooled = weighted.sum(dim=(1, 2))  # (C,)\n",
    "            else:\n",
    "                # Fallback to center pixel if patch size mismatch\n",
    "                pooled = feature_map[:, y, x]\n",
    "            \n",
    "            pooled_features.append(pooled)\n",
    "        \n",
    "        return torch.stack(pooled_features)  # (N, C)\n",
    "\n",
    "\n",
    "class GatedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Simplified Gated Attention for MetaSpace\"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, gate_type: str = \"headwise\", dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.gate_type = gate_type\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        if gate_type == \"headwise\":\n",
    "            self.W_gate = nn.Linear(d_model, num_heads)\n",
    "        else:\n",
    "            self.W_gate = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(B, N, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, N, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, N, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        Y = torch.matmul(attn, V)\n",
    "        \n",
    "        # Gating\n",
    "        gate_logits = self.W_gate(x)\n",
    "        gate_scores = torch.sigmoid(gate_logits)\n",
    "        \n",
    "        if self.gate_type == \"headwise\":\n",
    "            gate_scores = gate_scores.view(B, N, self.num_heads, 1).transpose(1, 2)\n",
    "        else:\n",
    "            gate_scores = gate_scores.view(B, N, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        Y_gated = Y * gate_scores\n",
    "        Y_gated = Y_gated.transpose(1, 2).contiguous().view(B, N, D)\n",
    "        \n",
    "        return self.W_o(Y_gated)\n",
    "\n",
    "\n",
    "class MetaSpace(nn.Module):\n",
    "    \"\"\"\n",
    "    MetaSpace: 학습 중 keypoint feature를 저장하고 업데이트하는 메타 공간\n",
    "    \n",
    "    각 feature level과 keypoint마다 평균 feature를 유지하며,\n",
    "    Gated Attention을 통해 meta feature와 current feature를 융합\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 original_size: Tuple[int, int],\n",
    "                 feature_dims: List[int],  # [256, 512, 1024] 같은 각 레벨의 feature dimension\n",
    "                 num_kpts: int,\n",
    "                 num_heads: int = 8,\n",
    "                 momentum: float = 0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_size: 원본 이미지 크기 (H, W)\n",
    "            feature_dims: 각 feature level의 dimension 리스트\n",
    "            num_kpts: keypoint 개수\n",
    "            num_heads: attention head 수\n",
    "            momentum: EMA 업데이트 momentum (0.9면 90% old, 10% new)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.original_size = original_size\n",
    "        self.num_kpts = num_kpts\n",
    "        self.num_levels = len(feature_dims)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Gaussian pooling for local feature extraction\n",
    "        self.pool = GaussianPooling(kernel_size=5, sigma=2.0)\n",
    "        \n",
    "        # Meta spaces: 각 level과 keypoint마다 learnable feature 저장\n",
    "        self.meta_spaces = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(num_kpts, feat_dim) * 0.02)  # 작은 값으로 초기화\n",
    "            for feat_dim in feature_dims\n",
    "        ])\n",
    "        \n",
    "        # Gated Multi-Head Attention for each level\n",
    "        self.gmha = nn.ModuleList([\n",
    "            GatedMultiHeadAttention(\n",
    "                d_model=feat_dim,\n",
    "                num_heads=num_heads,\n",
    "                gate_type=\"headwise\"\n",
    "            )\n",
    "            for feat_dim in feature_dims\n",
    "        ])\n",
    "        \n",
    "        # Projection layers (optional): feature fusion\n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(feat_dim * 2, feat_dim),\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(feat_dim, feat_dim)\n",
    "            )\n",
    "            for feat_dim in feature_dims\n",
    "        ])\n",
    "        \n",
    "        # Running statistics for accumulation (training only)\n",
    "        # Register as buffers (not parameters, won't be trained)\n",
    "        for level_idx in range(self.num_levels):\n",
    "            self.register_buffer(\n",
    "                f'feature_sum_{level_idx}',\n",
    "                torch.zeros(num_kpts, feature_dims[level_idx])\n",
    "            )\n",
    "            self.register_buffer(\n",
    "                f'feature_count_{level_idx}',\n",
    "                torch.zeros(num_kpts)\n",
    "            )\n",
    "    \n",
    "    def cal_resized_keypoints(self, \n",
    "                             keypoints: torch.Tensor, \n",
    "                             target_size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Resize keypoints to match feature map size\n",
    "        \n",
    "        Args:\n",
    "            keypoints: (B, N, 2) or (N, 2) in [x, y] format\n",
    "            target_size: (H, W) of feature map\n",
    "            \n",
    "        Returns:\n",
    "            resized_keypoints: same shape as input\n",
    "        \"\"\"\n",
    "        orig_h, orig_w = self.original_size\n",
    "        target_h, target_w = target_size\n",
    "        \n",
    "        scale_x = target_w / orig_w\n",
    "        scale_y = target_h / orig_h\n",
    "        \n",
    "        resized_kpts = keypoints.clone()\n",
    "        resized_kpts[..., 0] *= scale_x\n",
    "        resized_kpts[..., 1] *= scale_y\n",
    "        \n",
    "        return resized_kpts\n",
    "    \n",
    "    def extract_keypoint_features(self,\n",
    "                                  feature_map: torch.Tensor,\n",
    "                                  keypoints: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract features at keypoint locations using Gaussian pooling\n",
    "        \n",
    "        Args:\n",
    "            feature_map: (B, C, H, W) feature map\n",
    "            keypoints: (B, N, 2) keypoint coordinates [x, y]\n",
    "            \n",
    "        Returns:\n",
    "            kpt_features: (B, N, C) features at each keypoint\n",
    "        \"\"\"\n",
    "        B, C, H, W = feature_map.shape\n",
    "        _, N, _ = keypoints.shape\n",
    "        \n",
    "        # Resize keypoints to feature map scale\n",
    "        resized_kpts = self.cal_resized_keypoints(keypoints, (H, W))\n",
    "        \n",
    "        batch_features = []\n",
    "        for b in range(B):\n",
    "            # Extract features for this batch item\n",
    "            kpt_feats = self.pool(feature_map[b], resized_kpts[b])  # (N, C)\n",
    "            batch_features.append(kpt_feats)\n",
    "        \n",
    "        return torch.stack(batch_features)  # (B, N, C)\n",
    "    \n",
    "    def accumulate_features(self,\n",
    "                           level_idx: int,\n",
    "                           kpt_features: torch.Tensor,\n",
    "                           valid_mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Accumulate keypoint features for later update (training only)\n",
    "        \n",
    "        Args:\n",
    "            level_idx: feature level index\n",
    "            kpt_features: (B, N, C) extracted features\n",
    "            valid_mask: (B, N) boolean mask for valid keypoints\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return\n",
    "        \n",
    "        B = kpt_features.shape[0]\n",
    "        \n",
    "        feature_sum = getattr(self, f'feature_sum_{level_idx}')\n",
    "        feature_count = getattr(self, f'feature_count_{level_idx}')\n",
    "        \n",
    "        # Average over batch and accumulate\n",
    "        if valid_mask is not None:\n",
    "            # Only accumulate valid keypoints\n",
    "            valid_mask = valid_mask.float()  # (B, N)\n",
    "            masked_features = kpt_features * valid_mask.unsqueeze(-1)  # (B, N, C)\n",
    "            feature_sum += masked_features.sum(dim=0)  # (N, C)\n",
    "            feature_count += valid_mask.sum(dim=0)  # (N,)\n",
    "        else:\n",
    "            feature_sum += kpt_features.sum(dim=0)  # (N, C)\n",
    "            feature_count += B\n",
    "    \n",
    "    def update_meta_spaces(self):\n",
    "        \"\"\"\n",
    "        Update meta spaces with accumulated features (call at end of epoch/batch)\n",
    "        Uses momentum-based EMA update\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            feature_sum = getattr(self, f'feature_sum_{level_idx}')\n",
    "            feature_count = getattr(self, f'feature_count_{level_idx}')\n",
    "            \n",
    "            # Compute mean for keypoints with non-zero count\n",
    "            valid_kpts = feature_count > 0\n",
    "            \n",
    "            if valid_kpts.any():\n",
    "                # Mean features for valid keypoints\n",
    "                mean_features = torch.zeros_like(feature_sum)\n",
    "                mean_features[valid_kpts] = (\n",
    "                    feature_sum[valid_kpts] / feature_count[valid_kpts].unsqueeze(-1)\n",
    "                )\n",
    "                \n",
    "                # EMA update: new = momentum * old + (1 - momentum) * new\n",
    "                self.meta_spaces[level_idx].data[valid_kpts] = (\n",
    "                    self.momentum * self.meta_spaces[level_idx].data[valid_kpts] +\n",
    "                    (1 - self.momentum) * mean_features[valid_kpts]\n",
    "                )\n",
    "            \n",
    "            # Reset accumulators\n",
    "            feature_sum.zero_()\n",
    "            feature_count.zero_()\n",
    "    \n",
    "    def fuse_with_meta_features(self,\n",
    "                               level_idx: int,\n",
    "                               kpt_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fuse current keypoint features with meta space features using attention\n",
    "        \n",
    "        Args:\n",
    "            level_idx: feature level index\n",
    "            kpt_features: (B, N, C) current features\n",
    "            \n",
    "        Returns:\n",
    "            fused_features: (B, N, C) fused features\n",
    "        \"\"\"\n",
    "        B, N, C = kpt_features.shape\n",
    "        \n",
    "        # Get meta features for this level\n",
    "        meta_features = self.meta_spaces[level_idx].unsqueeze(0).expand(B, -1, -1)  # (B, N, C)\n",
    "        \n",
    "        # Concatenate current and meta features\n",
    "        combined = torch.cat([kpt_features, meta_features], dim=-1)  # (B, N, 2C)\n",
    "        \n",
    "        # Project and fuse\n",
    "        projected = self.projections[level_idx](combined)  # (B, N, C)\n",
    "        \n",
    "        # Apply gated attention for refinement\n",
    "        # Stack features for attention: [current, meta]\n",
    "        stacked = torch.stack([kpt_features, meta_features], dim=1)  # (B, 2, N, C)\n",
    "        stacked_reshaped = stacked.transpose(1, 2).reshape(B * N, 2, C)  # (B*N, 2, C)\n",
    "        \n",
    "        attended = self.gmha[level_idx](stacked_reshaped)  # (B*N, 2, C)\n",
    "        attended = attended.reshape(B, N, 2, C)  # (B, N, 2, C)\n",
    "        \n",
    "        # Weighted combination\n",
    "        fused = attended.mean(dim=2) + projected  # (B, N, C)\n",
    "        \n",
    "        return fused\n",
    "    \n",
    "    def forward(self, \n",
    "                feature_maps: List[torch.Tensor],\n",
    "                keypoints: torch.Tensor,\n",
    "                valid_mask: torch.Tensor = None) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass: extract, fuse, and accumulate keypoint features\n",
    "        \n",
    "        Args:\n",
    "            feature_maps: List of (B, C, H, W) feature maps from different levels\n",
    "            keypoints: (B, N, 2) keypoint coordinates [x, y] in original image space\n",
    "            valid_mask: (B, N) boolean mask for valid keypoints\n",
    "            \n",
    "        Returns:\n",
    "            fused_features: List of (B, N, C) fused features for each level\n",
    "        \"\"\"\n",
    "        assert len(feature_maps) == self.num_levels, \\\n",
    "            f\"Expected {self.num_levels} feature maps, got {len(feature_maps)}\"\n",
    "        \n",
    "        fused_features = []\n",
    "        \n",
    "        for level_idx, feature_map in enumerate(feature_maps):\n",
    "            # 1. Extract keypoint features\n",
    "            kpt_feats = self.extract_keypoint_features(feature_map, keypoints)\n",
    "            \n",
    "            # 2. Accumulate for meta space update (training only)\n",
    "            self.accumulate_features(level_idx, kpt_feats, valid_mask)\n",
    "            \n",
    "            # 3. Fuse with meta features\n",
    "            fused = self.fuse_with_meta_features(level_idx, kpt_feats)\n",
    "            \n",
    "            fused_features.append(fused)\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "\n",
    "# ===== Usage Example =====\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MetaSpace with Keypoint Features - Example\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    batch_size = 4\n",
    "    num_kpts = 17  # e.g., human pose keypoints\n",
    "    original_size = (256, 256)\n",
    "    \n",
    "    # Multi-scale feature maps (e.g., from backbone)\n",
    "    feature_maps = [\n",
    "        torch.randn(batch_size, 256, 64, 64),   # Level 0: 1/4 scale\n",
    "        torch.randn(batch_size, 512, 32, 32),   # Level 1: 1/8 scale\n",
    "        torch.randn(batch_size, 1024, 16, 16),  # Level 2: 1/16 scale\n",
    "    ]\n",
    "    \n",
    "    # Keypoints in original image coordinates\n",
    "    keypoints = torch.rand(batch_size, num_kpts, 2) * 256  # Random [0, 256]\n",
    "    \n",
    "    # Valid mask (e.g., some keypoints are occluded)\n",
    "    valid_mask = torch.rand(batch_size, num_kpts) > 0.2\n",
    "    \n",
    "    # Initialize MetaSpace\n",
    "    meta_space = MetaSpace(\n",
    "        original_size=original_size,\n",
    "        feature_dims=[256, 512, 1024],\n",
    "        num_kpts=num_kpts,\n",
    "        num_heads=8,\n",
    "        momentum=0.9\n",
    "    )\n",
    "    \n",
    "    print(\"\\n1. Initial forward pass (training mode):\")\n",
    "    meta_space.train()\n",
    "    fused_features = meta_space(feature_maps, keypoints, valid_mask)\n",
    "    \n",
    "    for i, feats in enumerate(fused_features):\n",
    "        print(f\"   Level {i}: {feats.shape}\")\n",
    "    \n",
    "    print(\"\\n2. Update meta spaces:\")\n",
    "    meta_space.update_meta_spaces()\n",
    "    print(\"   Meta spaces updated with accumulated features\")\n",
    "    \n",
    "    print(\"\\n3. Check meta space statistics:\")\n",
    "    for i, meta in enumerate(meta_space.meta_spaces):\n",
    "        print(f\"   Level {i} meta space: {meta.shape}\")\n",
    "        print(f\"      Mean: {meta.mean().item():.4f}, Std: {meta.std().item():.4f}\")\n",
    "    \n",
    "    print(\"\\n4. Inference mode:\")\n",
    "    meta_space.eval()\n",
    "    with torch.no_grad():\n",
    "        fused_features_eval = meta_space(feature_maps, keypoints)\n",
    "    print(\"   Features fused without accumulation\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"주요 기능:\")\n",
    "    print(\"- Multi-scale keypoint feature extraction\")\n",
    "    print(\"- Gaussian pooling for robust local features\")\n",
    "    print(\"- EMA-based meta feature learning\")\n",
    "    print(\"- Gated attention fusion\")\n",
    "    print(\"- Valid mask support for occluded keypoints\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSKD(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int\n",
    "        ):\n",
    "        super(FSKD, self).__init__()\n",
    "        self.backbone = torch.hub.load(\"models/backbone/dinov3\", \n",
    "                                       'dinov3_convnext_small', \n",
    "                                       source='local', \n",
    "                                       weights='./checkpoints/dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth')\n",
    "        \n",
    "        self.neck = nn.Sequential()\n",
    "        self.head = nn.Sequential()\n",
    "\n",
    "    def forward_features(\n",
    "            self, \n",
    "            x: torch.Tensor, \n",
    "            masks: Optional[torch.Tensor] = None\n",
    "        ) -> List[Dict[str, torch.Tensor]]:\n",
    "        scaled_features = self.backbone.forward_features_list([x], [masks])[1:]\n",
    "        pose_feature = self.neck(scaled_features)\n",
    "        result = self.head(pose_feature)\n",
    "        return result\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> List[Dict[str, torch.Tensor]]:\n",
    "        result = self.forward_features(x)\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
