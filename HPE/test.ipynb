{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dbd828",
   "metadata": {},
   "source": [
    "# Dinov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5321006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/otter/.local/share/mamba/envs/hpe/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import torch\n",
    "from models.pose import DINOv3Pose\n",
    "\n",
    "def preprocess(image_input, device='cuda'):\n",
    "    \"\"\"이미지 로드, 리사이징, 패딩, 정규화\"\"\"\n",
    "    # 1. 입력 처리 (경로 or Tensor)\n",
    "    if isinstance(image_input, str):\n",
    "        img = cv2.imread(image_input)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not found: {image_input}\")\n",
    "    elif isinstance(image_input, np.ndarray):\n",
    "        img = image_input\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a file path(str) or numpy array\")\n",
    "\n",
    "    h0, w0 = img.shape[:2]\n",
    "    \n",
    "    # 2. Resize (비율 유지)\n",
    "    target_size = 640\n",
    "    scale = min(target_size / h0, target_size / w0)\n",
    "    h, w = int(h0 * scale), int(w0 * scale)\n",
    "    \n",
    "    img_resized = cv2.resize(img, (w, h))\n",
    "    \n",
    "    # 3. Padding (32의 배수로 맞춤)\n",
    "    pad_h = (32 - h % 32) % 32\n",
    "    pad_w = (32 - w % 32) % 32\n",
    "    \n",
    "    # Right, Bottom 방향으로만 패딩 추가\n",
    "    img_padded = cv2.copyMakeBorder(\n",
    "        img_resized, 0, pad_h, 0, pad_w, \n",
    "        cv2.BORDER_CONSTANT, value=(114, 114, 114)\n",
    "    )\n",
    "    \n",
    "    # 4. To Tensor\n",
    "    img_tensor = torch.from_numpy(img_padded).permute(2, 0, 1).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # 복원용 메타 데이터\n",
    "    meta = {\n",
    "        'orig_shape': (h0, w0),\n",
    "        'scale': scale,\n",
    "        'pad': (0, pad_h, 0, pad_w) # top, bottom, left, right\n",
    "    }\n",
    "    \n",
    "    return img_tensor, img, meta\n",
    "\n",
    "model = DINOv3Pose('dinov3_convnext_small', finetuning=True)\n",
    "img = preprocess('./examples/00017.png')[0]\n",
    "result = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40b2360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3736 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3736/3736 [00:01<00:00, 3632.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Sorted labels are saved in: /media/otter/otterHD/AXData/TotalAX/TotalAX/valid/labels_sorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_sorted_indices(kpts_xy):\n",
    "    \"\"\"\n",
    "    키포인트 좌표(x, y)를 받아서 정렬된 인덱스를 반환합니다.\n",
    "    규칙:\n",
    "    1. 중심점 기준 시계 방향 정렬\n",
    "    2. 시작점은 '중심보다 오른쪽에 있는 점 중 가장 위에 있는 점(Y가 작은 값 or 큰 값)'\n",
    "       (주의: 이미지 좌표계에서는 Y가 0일수록 '위'입니다. 아래 코드는 Y가 작을수록 위라고 가정합니다.)\n",
    "       (만약 일반 수학 좌표계라면 argmin을 argmax로 바꾸세요.)\n",
    "    \"\"\"\n",
    "    pts = np.array(kpts_xy)\n",
    "    if len(pts) == 0:\n",
    "        return []\n",
    "\n",
    "    center = np.mean(pts, axis=0)\n",
    "    \n",
    "    # 1. 각도 계산 (이미지 좌표계: y가 아래로 증가하므로 y축 반전 고려)\n",
    "    # 이미지 상에서 시계방향: 12시 -> 3시 -> 6시\n",
    "    # 일반 수학 좌표계(y상승) arctan2: 반시계.\n",
    "    # 이미지 좌표계(y하강)에서 arctan2(y, x)를 쓰면:\n",
    "    # y가 양수(아래)일수록 각도가 커짐 -> 시계방향과 유사하게 작동\n",
    "    angles = np.arctan2(pts[:, 1] - center[1], pts[:, 0] - center[0])\n",
    "    \n",
    "    # 각도 기준 정렬\n",
    "    sorted_indices = np.argsort(angles)\n",
    "    sorted_pts = pts[sorted_indices]\n",
    "    \n",
    "    # 2. 시작점(Pivot) 찾기: 중심보다 오른쪽(x > cx) 이면서 가장 위(y가 최소)\n",
    "    # YOLO 정규화 좌표는 위쪽이 0, 아래쪽이 1입니다. 따라서 '가장 위' = 'y값이 최소'\n",
    "    rel_x = sorted_pts[:, 0] - center[0]\n",
    "    \n",
    "    # 오른쪽 점들 마스크\n",
    "    right_mask = rel_x > 0\n",
    "    if not np.any(right_mask):\n",
    "        right_mask = np.ones(len(sorted_pts), dtype=bool) # 예외: 오른쪽에 점이 없으면 전체 대상\n",
    "\n",
    "    # 후보 인덱스 (sorted_pts 기준)\n",
    "    candidate_local_indices = np.where(right_mask)[0]\n",
    "    \n",
    "    # 후보 중 Y값이 가장 작은 점 (이미지 상 가장 위)\n",
    "    candidate_y_values = sorted_pts[candidate_local_indices, 1]\n",
    "    best_candidate_idx = np.argmin(candidate_y_values) # <--- 중요: 이미지 좌표계는 min이 위쪽\n",
    "    \n",
    "    # 회전해야 할 양 (Shift amount)\n",
    "    pivot_index = candidate_local_indices[best_candidate_idx]\n",
    "    \n",
    "    # 인덱스 회전\n",
    "    final_indices = np.roll(sorted_indices, -pivot_index)\n",
    "    \n",
    "    return final_indices\n",
    "\n",
    "def process_yolo_labels(source_dir, target_dir, nkpts=4, dim=3):\n",
    "    \"\"\"\n",
    "    source_dir: 원본 txt 파일 경로\n",
    "    target_dir: 저장할 경로\n",
    "    nkpts: 키포인트 개수 (예: 4)\n",
    "    dim: 키포인트 당 값의 개수 (x,y,v면 3 / x,y면 2)\n",
    "    \"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    txt_files = glob.glob(os.path.join(source_dir, \"*.txt\"))\n",
    "    \n",
    "    print(f\"Processing {len(txt_files)} files...\")\n",
    "    \n",
    "    for txt_path in tqdm(txt_files):\n",
    "        filename = os.path.basename(txt_path)\n",
    "        save_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        new_lines = []\n",
    "        \n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5: # 데이터가 없거나 빈 줄\n",
    "                continue\n",
    "                \n",
    "            # 1. 기본 정보 분리 (cls, cx, cy, w, h)\n",
    "            cls_idx = parts[0]\n",
    "            bbox = parts[1:5] # cx, cy, w, h\n",
    "            \n",
    "            # 2. 키포인트 데이터 파싱\n",
    "            kpt_data = np.array(parts[5:], dtype=np.float32)\n",
    "            \n",
    "            # 데이터 개수가 맞는지 확인\n",
    "            if len(kpt_data) == nkpts * dim:\n",
    "                # (N, dim) 형태로 리쉐이프. 예: [[x1,y1,v1], [x2,y2,v2], ...]\n",
    "                kpts_reshaped = kpt_data.reshape(nkpts, dim)\n",
    "                \n",
    "                # 정렬을 위한 XY 좌표 추출\n",
    "                kpts_xy = kpts_reshaped[:, :2]\n",
    "                \n",
    "                # 3. 정렬 인덱스 계산\n",
    "                sorted_idx = get_sorted_indices(kpts_xy)\n",
    "                \n",
    "                # 4. 데이터 재배열 (Visibility 포함 전체 이동)\n",
    "                kpts_sorted = kpts_reshaped[sorted_idx]\n",
    "                \n",
    "                # 5. 다시 1줄짜리 문자열로 변환\n",
    "                kpts_flat = kpts_sorted.flatten()\n",
    "                kpts_str = \" \".join([f\"{x:.6f}\" for x in kpts_flat]) # 소수점 6자리 포맷팅\n",
    "                \n",
    "                # bbox 정보와 합치기\n",
    "                bbox_str = \" \".join(bbox)\n",
    "                new_line = f\"{cls_idx} {bbox_str} {kpts_str}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "            else:\n",
    "                # 키포인트 개수가 안 맞으면 건드리지 않고 원본 유지 (혹은 에러처리)\n",
    "                print(f\"Warning: {filename} has invalid keypoint length. Skipping line.\")\n",
    "                new_lines.append(line)\n",
    "        \n",
    "        # 저장\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.writelines(new_lines)\n",
    "\n",
    "    print(\"Done! Sorted labels are saved in:\", target_dir)\n",
    "\n",
    "# ==================================================\n",
    "# 실행 설정\n",
    "# ==================================================\n",
    "source_folder = '/media/otter/otterHD/AXData/TotalAX/TotalAX/valid/labels'\n",
    "target_folder = '/media/otter/otterHD/AXData/TotalAX/TotalAX/valid/labels_sorted'\n",
    "\n",
    "# 내 데이터 설정에 맞게 수정하세요\n",
    "NUM_KEYPOINTS = 4  # 점의 개수\n",
    "DIM_PER_POINT = 2  # 점 하나당 값의 개수 (x,y,v = 3 / x,y = 2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_yolo_labels(source_folder, target_folder, nkpts=NUM_KEYPOINTS, dim=DIM_PER_POINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f2664fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from models.backbones.dinov3convnext import Dinov3ConvNext, convnext_sizes\n",
    "\n",
    "convnext_ckps = {\n",
    "    'tiny': './checkpoints/dinov3/convnext/dinov3_convnext_tiny_pretrain_lvd1689m-21b726bb.pth', \n",
    "    'small': './checkpoints/dinov3/convnext/dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth', \n",
    "    'base': './checkpoints/dinov3/convnext/dinov3_convnext_base_pretrain_lvd1689m-801f2ba9.pth', \n",
    "    'large': './checkpoints/dinov3/convnext/dinov3_convnext_large_pretrain_lvd1689m-61fa432d.pth'}\n",
    "\n",
    "vit_ckps = {\n",
    "    'small': './checkpoints/dinov3/vit/dinov3_vits16_pretrain_lvd1689m-08c60483.pth', \n",
    "    'base': './checkpoints/dinov3/vit/dinov3_vit7b16_pretrain_lvd1689m-a955f4ea.pth', \n",
    "    'large': None}\n",
    "\n",
    "model_size = 'small'\n",
    "\n",
    "model = Dinov3ConvNext(\n",
    "    depths=convnext_sizes[model_size][\"depths\"],\n",
    "    dims=convnext_sizes[model_size][\"dims\"],    \n",
    "    weights=convnext_ckps[model_size],\n",
    "    )\n",
    "inputs = torch.randn(3, 3, 640, 640)\n",
    "feature_list = model.forward_features([inputs], [None])\n",
    "output = model(inputs)\n",
    "print(output.shape)\n",
    "feature_list['x_norm_patchtokens'].shape\n",
    "\n",
    "# model = Dinov3ViT(\n",
    "#     patch_size=vit_sizes[model_size][\"patch_size\"],\n",
    "#     embed_dim=vit_sizes[model_size][\"embed_dim\"],\n",
    "#     depth=vit_sizes[model_size][\"depth\"],\n",
    "#     num_heads=vit_sizes[model_size][\"num_heads\"],\n",
    "#     ffn_ratio=vit_sizes[model_size][\"ffn_ratio\"],\n",
    "#     weights=vit_ckps[model_size],\n",
    "#     )\n",
    "inputs = torch.randn(3, 3, 640, 640)\n",
    "feature_list = model.forward_features_list([inputs], [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2af7294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 192, 80, 80])\n",
      "torch.Size([3, 384, 40, 40])\n",
      "torch.Size([3, 768, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    print(feature_list[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211196c5",
   "metadata": {},
   "source": [
    "# Meta Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MetaSpace with Keypoint Features - Example\n",
      "============================================================\n",
      "\n",
      "1. Initial forward pass (training mode):\n",
      "   Level 0: torch.Size([4, 17, 256])\n",
      "   Level 1: torch.Size([4, 17, 512])\n",
      "   Level 2: torch.Size([4, 17, 1024])\n",
      "\n",
      "2. Update meta spaces:\n",
      "   Meta spaces updated with accumulated features\n",
      "\n",
      "3. Check meta space statistics:\n",
      "   Level 0 meta space: torch.Size([17, 256])\n",
      "      Mean: 0.0005, Std: 0.0774\n",
      "   Level 1 meta space: torch.Size([17, 512])\n",
      "      Mean: -0.0006, Std: 0.0565\n",
      "   Level 2 meta space: torch.Size([17, 1024])\n",
      "      Mean: -0.0000, Std: 0.0411\n",
      "\n",
      "4. Inference mode:\n",
      "   Features fused without accumulation\n",
      "\n",
      "============================================================\n",
      "주요 기능:\n",
      "- Multi-scale keypoint feature extraction\n",
      "- Gaussian pooling for robust local features\n",
      "- EMA-based meta feature learning\n",
      "- Gated attention fusion\n",
      "- Valid mask support for occluded keypoints\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from models.backbones import Dinov3ViT, vit_sizes\n",
    "from models.modules import MetaSpace\n",
    "\n",
    "vit_ckps = {\n",
    "    'small': './checkpoints/dinov3/vit/dinov3_vits16_pretrain_lvd1689m-08c60483.pth', \n",
    "    'base': './checkpoints/dinov3/vit/dinov3_vit7b16_pretrain_lvd1689m-a955f4ea.pth', \n",
    "    'large': None}\n",
    "\n",
    "model_size = 'small'\n",
    "\n",
    "# ===== Usage Example =====\n",
    "model = Dinov3ViT(\n",
    "    patch_size=vit_sizes[model_size][\"patch_size\"],\n",
    "    embed_dim=vit_sizes[model_size][\"embed_dim\"],\n",
    "    depth=vit_sizes[model_size][\"depth\"],\n",
    "    num_heads=vit_sizes[model_size][\"num_heads\"],\n",
    "    ffn_ratio=vit_sizes[model_size][\"ffn_ratio\"],\n",
    "    weights=vit_ckps[model_size],\n",
    "    )\n",
    "print(\"=\" * 60)\n",
    "print(\"MetaSpace with Keypoint Features - Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "num_kpts = 17  # e.g., human pose keypoints\n",
    "original_size = (256, 256)\n",
    "\n",
    "# Multi-scale feature maps (e.g., from backbone)\n",
    "feature_maps = [\n",
    "    torch.randn(batch_size, 256, 64, 64),   # Level 0: 1/4 scale\n",
    "    torch.randn(batch_size, 512, 32, 32),   # Level 1: 1/8 scale\n",
    "    torch.randn(batch_size, 1024, 16, 16),  # Level 2: 1/16 scale\n",
    "]\n",
    "\n",
    "# Keypoints in original image coordinates\n",
    "keypoints = torch.rand(batch_size, num_kpts, 2) * 256  # Random [0, 256]\n",
    "\n",
    "# Valid mask (e.g., some keypoints are occluded)\n",
    "valid_mask = torch.rand(batch_size, num_kpts) > 0.2\n",
    "\n",
    "# Initialize MetaSpace\n",
    "meta_space = MetaSpace(\n",
    "    original_size=original_size,\n",
    "    feature_dims=[256, 512, 1024],\n",
    "    num_kpts=num_kpts,\n",
    "    num_heads=8,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "print(\"\\n1. Initial forward pass (training mode):\")\n",
    "meta_space.train()\n",
    "fused_features = meta_space(feature_maps, keypoints, valid_mask)\n",
    "\n",
    "for i, feats in enumerate(fused_features):\n",
    "    print(f\"   Level {i}: {feats.shape}\")\n",
    "\n",
    "print(\"\\n2. Update meta spaces:\")\n",
    "meta_space.update_meta_spaces()\n",
    "print(\"   Meta spaces updated with accumulated features\")\n",
    "\n",
    "print(\"\\n3. Check meta space statistics:\")\n",
    "for i, meta in enumerate(meta_space.meta_spaces):\n",
    "    print(f\"   Level {i} meta space: {meta.shape}\")\n",
    "    print(f\"      Mean: {meta.mean().item():.4f}, Std: {meta.std().item():.4f}\")\n",
    "\n",
    "print(\"\\n4. Inference mode:\")\n",
    "meta_space.eval()\n",
    "with torch.no_grad():\n",
    "    fused_features_eval = meta_space(feature_maps, keypoints)\n",
    "print(\"   Features fused without accumulation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"주요 기능:\")\n",
    "print(\"- Multi-scale keypoint feature extraction\")\n",
    "print(\"- Gaussian pooling for robust local features\")\n",
    "print(\"- EMA-based meta feature learning\")\n",
    "print(\"- Gated attention fusion\")\n",
    "print(\"- Valid mask support for occluded keypoints\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90408d",
   "metadata": {},
   "source": [
    "# FSKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from models.backbones.dinov3vit import Dinov3ViT, vit_sizes\n",
    "\n",
    "class FSKD(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nkpts: int,\n",
    "            backbone: str = 'base',\n",
    "            pretrained: bool = True,\n",
    "        ):\n",
    "        super(FSKD, self).__init__()\n",
    "        backbone_cfg = vit_sizes[backbone]\n",
    "        embed_dim = backbone_cfg['embed_dim']\n",
    "\n",
    "        self.backbone = Dinov3ViT        (\n",
    "            patch_size=backbone_cfg[\"patch_size\"],\n",
    "            embed_dim=embed_dim,\n",
    "            depth=backbone_cfg[\"depth\"],\n",
    "            num_heads=backbone_cfg[\"num_heads\"],\n",
    "            ffn_ratio=backbone_cfg[\"ffn_ratio\"],\n",
    "            pretrained=pretrained,\n",
    "        )\n",
    "\n",
    "        self.neck = nn.Identity()\n",
    "        self.head = nn.Linear(embed_dim, nkpts * 2)\n",
    "\n",
    "    def forward_features(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            masks: Optional[torch.Tensor] = None\n",
    "        ) -> torch.Tensor:\n",
    "        _, all_xes = self.backbone.forward_features_list([x], [masks])\n",
    "        \n",
    "        last_block_features = all_xes[-1][0]\n",
    "        cls_token = last_block_features[:, 0]\n",
    "\n",
    "        pose_feature = self.neck(cls_token)\n",
    "        result = self.head(pose_feature)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.forward_features(x)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c69aa",
   "metadata": {},
   "source": [
    "# Autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f16f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.amp.autocast_mode.autocast at 0x76d12b671e10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.amp as amp \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "amp.autocast(device, enabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546e771",
   "metadata": {},
   "source": [
    "# DeepPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30024a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import timm.models.resnet as resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73765626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.resnet import Bottleneck\n",
    "from timm.models.resnet import BasicBlock\n",
    "from timm.models.helpers import load_pretrained, resolve_pretrained_cfg\n",
    "\n",
    "resnet_args = {\n",
    "    'resnet18': dict(block=BasicBlock, layers=(2, 2, 2, 2)),\n",
    "    'resnet34': dict(block=BasicBlock, layers=(3, 4, 6, 3)),\n",
    "    'resnet50': dict(block=Bottleneck, layers=(3, 4, 6, 3)),\n",
    "    'resnet50s': dict(block=Bottleneck, layers=(3, 4, 6, 3), stem_width=64, stem_type='deep'),\n",
    "    'resnet50t': dict(block=Bottleneck, layers=(3, 4, 6, 3), stem_width=32, stem_type='deep_tiered', avg_down=True),\n",
    "    'resnet101': dict(block=Bottleneck, layers=(3, 4, 23, 3))\n",
    "}\n",
    "\n",
    "resnet_ckps = {\n",
    "    'resnet18': './checkpoints/resnet/resnet18-5c106cde.pth',\n",
    "    'resnet34': './checkpoints/resnet/resnet34-333f7ec4.pth',\n",
    "    'resnet50': './checkpoints/resnet/resnet50-0676ba61.pth',\n",
    "    'resnet50s': './checkpoints/resnet/resnet50s-3cf99910.pth',\n",
    "    'resnet50t': './checkpoints/resnet/resnet50t-1f8793b8.pth',\n",
    "    'resnet101': './checkpoints/resnet/resnet101-5d3b4d8f.pth'\n",
    "}\n",
    "\n",
    "class DeepPose(timm.models.ResNet):\n",
    "    def __init__(self, num_joints=8, backbone='resnet50', checkpoint=None, download=False, **kwargs):\n",
    "        model_args = resnet_args.get(backbone)\n",
    "        super(DeepPose, self).__init__(**dict(model_args, **kwargs))\n",
    "        features = False\n",
    "\n",
    "        # resolve and update model pretrained config and model kwargs\n",
    "        pretrained_cfg = resolve_pretrained_cfg(\n",
    "            backbone,\n",
    "            pretrained_cfg=None,\n",
    "            pretrained_cfg_overlay=None\n",
    "        )\n",
    "        pretrained_cfg = pretrained_cfg.to_dict()\n",
    "        \n",
    "        # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n",
    "        num_classes_pretrained = 0 if features else getattr(self, 'num_classes', kwargs.get('num_classes', 1000))\n",
    "        if download:\n",
    "            checkpoint = load_pretrained(\n",
    "                timm.models.ResNet,\n",
    "                pretrained_cfg=pretrained_cfg,\n",
    "                num_classes=num_classes_pretrained,\n",
    "                in_chans=kwargs.get('in_chans', 3),\n",
    "                filter_fn=None,\n",
    "                strict=None,\n",
    "                cache_dir=None,\n",
    "                )\n",
    "        print(checkpoint)\n",
    "        if checkpoint:\n",
    "            self.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "        self.njoints = num_joints\n",
    "        self.fc = nn.Linear(self.fc.in_features, self.njoints*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f08b51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.load_state_dict() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDeepPose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m random_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[51], line 40\u001b[0m, in \u001b[0;36mDeepPose.__init__\u001b[0;34m(self, num_joints, backbone, checkpoint, download, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m num_classes_pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 40\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResNet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_chans\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(checkpoint)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint:\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/hpe/lib/python3.10/site-packages/timm/models/_builder.py:280\u001b[0m, in \u001b[0;36mload_pretrained\u001b[0;34m(model, pretrained_cfg, num_classes, in_chans, filter_fn, strict, cache_dir)\u001b[0m\n\u001b[1;32m    277\u001b[0m             classifier_bias \u001b[38;5;241m=\u001b[39m state_dict[classifier_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    278\u001b[0m             state_dict[classifier_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m classifier_bias[label_offset:]\n\u001b[0;32m--> 280\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_result\u001b[38;5;241m.\u001b[39mmissing_keys:\n\u001b[1;32m    282\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing keys (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(load_result\u001b[38;5;241m.\u001b[39mmissing_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) discovered while loading pretrained weights.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m This is expected if model is being adapted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.load_state_dict() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    import torch \n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = DeepPose(8, 'resnet18', download=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    random_noise = torch.randn((32, 3, 224, 224)).to(device)\n",
    "    result = model(random_noise)\n",
    "\n",
    "    print(f'INPUT SIZE : {random_noise.shape}')\n",
    "    print(f'OUTPUT SIZE : {result.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc21e1",
   "metadata": {},
   "source": [
    "# FCMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1906056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import spconv.pytorch as spconv\n",
    "\n",
    "class SpconvGRN(nn.Module):\n",
    "    \"\"\" GRN layer for spconv tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # 파라미터 정의는 동일합니다.\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim))\n",
    "\n",
    "    def forward(self, x: spconv.SparseConvTensor):\n",
    "        # 1. spconv 텐서에서 Dense Feature(N, C)를 꺼냅니다.\n",
    "        # MinkowskiEngine의 x.F와 동일합니다.\n",
    "        features = x.features \n",
    "\n",
    "        # 2. GRN 연산 수행 (PyTorch 연산이므로 완전히 동일합니다)\n",
    "        # 주의: dim=0은 배치 내 모든 포인트에 대해 통계를 집계합니다. (원본 코드 동작 유지)\n",
    "        Gx = torch.norm(features, p=2, dim=0, keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # 공식: gamma * (X * Nx) + beta + X\n",
    "        out_features = self.gamma * (features * Nx) + self.beta + features\n",
    "\n",
    "        # 3. spconv 방식의 반환\n",
    "        # 새로운 텐서를 밑바닥부터 만들지 않고, \n",
    "        # 기존 텐서(x)의 구조(좌표, shape 등)는 유지하되 피처만 갈아끼웁니다.\n",
    "        return x.replace_feature(out_features)\n",
    "    \n",
    "\n",
    "class SpconvLayerNorm(nn.Module):\n",
    "    \"\"\" Channel-wise layer normalization for spconv tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape,\n",
    "        eps=1e-6,\n",
    "    ):\n",
    "        super(SpconvLayerNorm, self).__init__()\n",
    "        # nn.LayerNorm은 (N, C) 2D 텐서를 잘 처리하므로 그대로 씁니다.\n",
    "        self.ln = nn.LayerNorm(normalized_shape, eps=eps)\n",
    "\n",
    "    def forward(self, input: spconv.SparseConvTensor):\n",
    "        # 1. spconv 텐서에서 피처 추출 (.F -> .features)\n",
    "        output_features = self.ln(input.features)\n",
    "        \n",
    "        # 2. 좌표 등 구조는 유지하고 피처만 교체하여 반환\n",
    "        return input.replace_feature(output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9cb38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features:\n",
      "tensor([-0.2196,  0.0146,  0.2861,  0.3721, -0.8692], device='cuda:0')... (Shape: torch.Size([4, 32]))\n",
      "--------------------------------------------------\n",
      "running LayerNorm...\n",
      "LN Output:\n",
      "tensor([-0.1184,  0.0743,  0.2975,  0.3683, -0.6527], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)... (Shape: torch.Size([4, 32]))\n",
      "--------------------------------------------------\n",
      "running GRN...\n",
      "GRN Output:\n",
      "tensor([-0.1184,  0.0743,  0.2975,  0.3683, -0.6527], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)... (Shape: torch.Size([4, 32]))\n",
      "--------------------------------------------------\n",
      "Structure Check:\n",
      "Indices preserved? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import spconv.pytorch as spconv\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1. 작성하신 클래스 정의 (위에서 만드신 코드)\n",
    "# --------------------------------------------------------\n",
    "class SpconvGRN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim))\n",
    "\n",
    "    def forward(self, x: spconv.SparseConvTensor):\n",
    "        features = x.features \n",
    "        # dim=0: 모든 복셀(N)에 대해 통계 집계 (Global Context)\n",
    "        Gx = torch.norm(features, p=2, dim=0, keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        out_features = self.gamma * (features * Nx) + self.beta + features\n",
    "        return x.replace_feature(out_features)\n",
    "\n",
    "class SpconvLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super(SpconvLayerNorm, self).__init__()\n",
    "        self.ln = nn.LayerNorm(normalized_shape, eps=eps)\n",
    "\n",
    "    def forward(self, input: spconv.SparseConvTensor):\n",
    "        output_features = self.ln(input.features)\n",
    "        return input.replace_feature(output_features)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2. 실행 예시 (Input 만들기 & Forward)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # 설정 값\n",
    "    batch_size = 2\n",
    "    in_channels = 32\n",
    "    spatial_shape = [100, 100, 100] # [Z, Y, X] 크기\n",
    "\n",
    "    # 1) 좌표 (Indices) 생성 - [Batch_idx, Z, Y, X]\n",
    "    # 주의: spconv는 반드시 **int32** 타입을 사용해야 합니다!\n",
    "    indices = torch.tensor([\n",
    "        [0, 10, 20, 30],  # Batch 0번의 점\n",
    "        [0, 10, 20, 31],  # Batch 0번의 점 (바로 옆)\n",
    "        [1, 50, 50, 50],  # Batch 1번의 점\n",
    "        [1, 51, 51, 51],  # Batch 1번의 점\n",
    "    ], dtype=torch.int32).cuda() # GPU 사용 시 .cuda() 필수\n",
    "\n",
    "    # 2) 피처 (Features) 생성 - (N, C)\n",
    "    # 점이 4개이므로 N=4\n",
    "    features = torch.randn(4, in_channels).cuda()\n",
    "\n",
    "    # 3) SparseConvTensor 생성\n",
    "    # ME와 달리 spatial_shape와 batch_size를 꼭 넣어줘야 합니다.\n",
    "    input_tensor = spconv.SparseConvTensor(\n",
    "        features=features,\n",
    "        indices=indices,\n",
    "        spatial_shape=spatial_shape,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    print(f\"Input Features:\\n{input_tensor.features[0][:5]}... (Shape: {input_tensor.features.shape})\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 4) LayerNorm 테스트\n",
    "    ln_layer = SpconvLayerNorm(in_channels).cuda()\n",
    "    out_ln = ln_layer(input_tensor)\n",
    "    \n",
    "    print(\"running LayerNorm...\")\n",
    "    print(f\"LN Output:\\n{out_ln.features[0][:5]}... (Shape: {out_ln.features.shape})\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 5) GRN 테스트\n",
    "    grn_layer = SpconvGRN(in_channels).cuda()\n",
    "    out_grn = grn_layer(out_ln)\n",
    "\n",
    "    print(\"running GRN...\")\n",
    "    print(f\"GRN Output:\\n{out_grn.features[0][:5]}... (Shape: {out_grn.features.shape})\")\n",
    "    \n",
    "    # 6) 구조가 유지되었는지 확인 (Indices가 바뀌면 안됨)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Structure Check:\")\n",
    "    is_same = torch.equal(input_tensor.indices, out_grn.indices)\n",
    "    print(f\"Indices preserved? {is_same}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CUDA가 사용 가능할 때만 실행 (spconv는 기본적으로 CUDA 의존성이 강함)\n",
    "    if torch.cuda.is_available():\n",
    "        main()\n",
    "    else:\n",
    "        print(\"CUDA device not found. spconv requires GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0299196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spconv.pytorch as spconv\n",
    "import spconv.pytorch.quantization as spconvq\n",
    "from spconv.pytorch.quantization import get_default_spconv_trt_ptq_qconfig\n",
    "from spconv.pytorch.quantization.core import quantize_per_tensor\n",
    "from spconv.pytorch.quantization.fake_q import \\\n",
    "    get_default_spconv_qconfig_mapping\n",
    "from spconv.pytorch.quantization.intrinsic.modules import SpconvBnAddReLUNd, SpconvAddReLUNd\n",
    "import spconv.pytorch.quantization.intrinsic.quantized as snniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3925c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "class SpconvGRN(spconv.SparseModule):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim))\n",
    "\n",
    "    def forward(self, x: spconv.SparseConvTensor):\n",
    "        features = x.features \n",
    "        # dim=0: 모든 복셀(N)에 대해 통계 집계 (Global Context)\n",
    "        Gx = torch.norm(features, p=2, dim=0, keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        out_features = self.gamma * (features * Nx) + self.beta + features\n",
    "        return out_features\n",
    "\n",
    "class SpconvLayerNorm(spconv.SparseModule):\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super(SpconvLayerNorm, self).__init__()\n",
    "        self.ln = nn.LayerNorm(normalized_shape, eps=eps)\n",
    "\n",
    "    def forward(self, input: spconv.SparseConvTensor):\n",
    "        output_features = self.ln(input.features)\n",
    "        return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539d02d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Input shape: torch.Size([2, 3, 224, 224])\n",
      "Number of patches: 3136\n",
      "Mask shape: torch.Size([2, 3136])\n",
      "Mask ratio: 49.84%\n",
      "\n",
      "Forward pass...\n",
      "❌ Error: The size of tensor a (3146) must match the size of tensor b (6272) at non-singleton dimension 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_115957/2694884605.py\", line 352, in <module>\n",
      "    output = model(x, mask)\n",
      "  File \"/home/otter/.local/share/mamba/envs/hpe/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/otter/.local/share/mamba/envs/hpe/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_115957/2694884605.py\", line 314, in forward\n",
      "    x = self.stages[i](x)\n",
      "  File \"/home/otter/.local/share/mamba/envs/hpe/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/otter/.local/share/mamba/envs/hpe/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/otter/.local/share/mamba/envs/hpe/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/otter/.local/share/mamba/envs/hpe/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/otter/.local/share/mamba/envs/hpe/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_115957/2694884605.py\", line 223, in forward\n",
      "    x = x.replace_feature(shortcut.features + x.features)\n",
      "RuntimeError: The size of tensor a (3146) must match the size of tensor b (6272) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import spconv.pytorch as spconv\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "# 필요한 helper 함수들\n",
    "def to_sparse(x):\n",
    "    \"\"\"Dense tensor를 sparse tensor로 변환\"\"\"\n",
    "    batch_size, in_channels, H, W = x.shape\n",
    "    \n",
    "    # Non-zero 위치 찾기\n",
    "    mask = (x.abs().sum(dim=1) > 1e-6)  # [B, H, W]\n",
    "    \n",
    "    indices_list = []\n",
    "    features_list = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        nz_indices = torch.nonzero(mask[b], as_tuple=False)  # [N, 2] (y, x)\n",
    "        \n",
    "        if len(nz_indices) > 0:\n",
    "            batch_indices = torch.full((len(nz_indices), 1), b, dtype=torch.int32, device=x.device)\n",
    "            batch_nz_indices = torch.cat([batch_indices, nz_indices.int()], dim=1)\n",
    "            indices_list.append(batch_nz_indices)\n",
    "            \n",
    "            y_coords = nz_indices[:, 0]\n",
    "            x_coords = nz_indices[:, 1]\n",
    "            feats = x[b, :, y_coords, x_coords].T  # [N, C]\n",
    "            features_list.append(feats)\n",
    "    \n",
    "    if len(indices_list) > 0:\n",
    "        indices = torch.cat(indices_list, dim=0)\n",
    "        features = torch.cat(features_list, dim=0)\n",
    "    else:\n",
    "        indices = torch.zeros((0, 3), dtype=torch.int32, device=x.device)\n",
    "        features = torch.zeros((0, in_channels), dtype=x.dtype, device=x.device)\n",
    "    \n",
    "    sparse_tensor = spconv.SparseConvTensor(\n",
    "        features=features,\n",
    "        indices=indices,\n",
    "        spatial_shape=[H, W],\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return sparse_tensor\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm for channels_first format\"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "class SpLayerNorm(nn.Module):\n",
    "    \"\"\"Sparse LayerNorm\"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, spconv.SparseConvTensor):\n",
    "            features = x.features\n",
    "            features = nn.functional.layer_norm(features, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "            return x.replace_feature(features)\n",
    "        else:\n",
    "            return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "\n",
    "\n",
    "class SparseGELU(nn.Module):\n",
    "    \"\"\"Sparse GELU activation\"\"\"\n",
    "    def __init__(self, approximate='none'):\n",
    "        super().__init__()\n",
    "        self.gelu = nn.GELU(approximate=approximate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, spconv.SparseConvTensor):\n",
    "            new_features = self.gelu(x.features)\n",
    "            return x.replace_feature(new_features)\n",
    "        else:\n",
    "            return self.gelu(x)\n",
    "\n",
    "\n",
    "class SpGRN(nn.Module):\n",
    "    \"\"\"Sparse Global Response Normalization\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, spconv.SparseConvTensor):\n",
    "            features = x.features  # [N, C]\n",
    "            # L2 norm per sample\n",
    "            gx = torch.norm(features, p=2, dim=1, keepdim=True)  # [N, 1]\n",
    "            nx = gx / (gx.mean(dim=0, keepdim=True) + 1e-6)\n",
    "            features = self.gamma * (features * nx) + self.beta + features\n",
    "            return x.replace_feature(features)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class SpDropPath(nn.Module):\n",
    "    \"\"\"Sparse Drop Path\"\"\"\n",
    "    def __init__(self, drop_prob=0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        \n",
    "        if isinstance(x, spconv.SparseConvTensor):\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            random_tensor = keep_prob + torch.rand((x.features.shape[0], 1), \n",
    "                                                   dtype=x.features.dtype, \n",
    "                                                   device=x.features.device)\n",
    "            random_tensor.floor_()  # binarize\n",
    "            features = x.features / keep_prob * random_tensor\n",
    "            return x.replace_feature(features)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class EfficientSparseDepthwiseConv2d(nn.Module):\n",
    "    \"\"\"효율적인 Sparse Depthwise Convolution\"\"\"\n",
    "    def __init__(self, channels, kernel_size=7, padding=3, bias=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        \n",
    "        # 7x7 depthwise를 위한 weight\n",
    "        self.weight = nn.Parameter(torch.randn(channels, kernel_size * kernel_size))\n",
    "        nn.init.kaiming_normal_(self.weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, spconv.SparseConvTensor):\n",
    "            raise TypeError(\"Input must be SparseConvTensor\")\n",
    "        \n",
    "        # Dense로 변환\n",
    "        dense = x.dense()  # [B, C, H, W]\n",
    "        B, C, H, W = dense.shape\n",
    "        \n",
    "        # Unfold: [B, C*K*K, H*W]\n",
    "        unfolded = self.unfold(dense)\n",
    "        unfolded = unfolded.view(B, C, -1, H*W)  # [B, C, K*K, H*W]\n",
    "        \n",
    "        # Depthwise\n",
    "        weight = self.weight.view(1, C, -1, 1)\n",
    "        out = (unfolded * weight).sum(dim=2)  # [B, C, H*W]\n",
    "        out = out.view(B, C, H, W)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias.view(1, -1, 1, 1)\n",
    "        \n",
    "        # 다시 sparse로 변환\n",
    "        return to_sparse(out)\n",
    "\n",
    "\n",
    "class SpBlock(nn.Module):\n",
    "    \"\"\"Sparse ConvNeXtV2 Block\"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dwconv = EfficientSparseDepthwiseConv2d(\n",
    "            channels=dim, \n",
    "            kernel_size=7, \n",
    "            padding=3, \n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        self.norm = SpLayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = spconv.SubMConv2d(dim, 4 * dim, kernel_size=1, bias=True)\n",
    "        self.act = SparseGELU()\n",
    "        self.grn = SpGRN(4 * dim)\n",
    "        self.pwconv2 = spconv.SubMConv2d(4 * dim, dim, kernel_size=1, bias=True)\n",
    "        \n",
    "        self.gamma = nn.Parameter(\n",
    "            layer_scale_init_value * torch.ones(dim),\n",
    "            requires_grad=True\n",
    "        ) if layer_scale_init_value > 0 else None\n",
    "        \n",
    "        self.drop_path = SpDropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        \n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        \n",
    "        if self.gamma is not None:\n",
    "            x = x.replace_feature(x.features * self.gamma.view(1, -1))\n",
    "        \n",
    "        x = self.drop_path(x)\n",
    "        x = x.replace_feature(shortcut.features + x.features)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SparseConvNeXtV2(nn.Module):\n",
    "    \"\"\"Sparse ConvNeXtV2\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_chans=3, \n",
    "                 num_classes=1000, \n",
    "                 depths=[3, 3, 9, 3], \n",
    "                 dims=[96, 192, 384, 768], \n",
    "                 drop_path_rate=0., \n",
    "                 D=2):\n",
    "        super().__init__()\n",
    "        self.depths = depths\n",
    "        self.num_classes = num_classes\n",
    "        self.D = D\n",
    "        \n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        \n",
    "        # Stem\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        \n",
    "        # Downsampling layers\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                SpLayerNorm(dims[i], eps=1e-6),\n",
    "                spconv.SparseConv2d(dims[i], dims[i+1], kernel_size=2, stride=2, bias=True)\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "        \n",
    "        # Stages\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[SpBlock(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (spconv.SubMConv2d, spconv.SparseConv2d)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def upsample_mask(self, mask, scale):\n",
    "        \"\"\"Mask upsampling\"\"\"\n",
    "        assert len(mask.shape) == 2\n",
    "        p = int(mask.shape[1] ** .5)\n",
    "        return mask.reshape(-1, p, p).\\\n",
    "                    repeat_interleave(scale, axis=1).\\\n",
    "                    repeat_interleave(scale, axis=2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Upsample mask to input resolution\n",
    "        mask = self.upsample_mask(mask, 4)\n",
    "        mask = mask.unsqueeze(1).type_as(x)\n",
    "        \n",
    "        # Apply mask\n",
    "        x = x * (1. - mask)\n",
    "        \n",
    "        # Stem\n",
    "        x = self.downsample_layers[0](x)\n",
    "        \n",
    "        # To sparse\n",
    "        x = to_sparse(x)\n",
    "        \n",
    "        # Sparse stages\n",
    "        for i in range(4):\n",
    "            if i > 0:\n",
    "                x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        \n",
    "        # Densify\n",
    "        x = x.dense()\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating model...\")\n",
    "    model = SparseConvNeXtV2(\n",
    "        in_chans=3,\n",
    "        num_classes=1000,\n",
    "        depths=[3, 3, 9, 3],\n",
    "        dims=[96, 192, 384, 768],\n",
    "        drop_path_rate=0.1,\n",
    "        D=2\n",
    "    )\n",
    "    \n",
    "    batch_size = 2\n",
    "    H, W = 224, 224\n",
    "    x = torch.randn(batch_size, 3, H, W)\n",
    "    \n",
    "    patch_size = 4\n",
    "    num_patches = (H // patch_size) * (W // patch_size)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Number of patches: {num_patches}\")\n",
    "    \n",
    "    mask = torch.rand(batch_size, num_patches) > 0.5\n",
    "    mask = mask.float()\n",
    "    \n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Mask ratio: {mask.mean().item():.2%}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nForward pass...\")\n",
    "        output = model(x, mask)\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        print(\"✅ Success!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce18cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: No module named 'tqdm'\n"
     ]
    }
   ],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3eb817",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unknown model (dinov3_vits14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdinov3_vits14\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/hpe/lib/python3.10/site-packages/timm/models/_factory.py:134\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, cache_dir, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         pretrained_cfg \u001b[38;5;241m=\u001b[39m pretrained_tag\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model(model_name):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown model (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model_name)\n\u001b[1;32m    136\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unknown model (dinov3_vits14)"
     ]
    }
   ],
   "source": [
    "timm.models.create_model('dinov3_vits14', pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
