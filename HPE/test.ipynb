{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e34c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "backbone = torch.hub.load(\"models/backbone/dinov3\", 'dinov3_convnext_small', source='local', weights='./checkpoints/dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth')\n",
    "backbone.forward_features(torch.randn(1, 3, 224, 224))[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPooling(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7, sigma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # Create Gaussian kernel\n",
    "        ax = torch.arange(-self.padding, self.padding + 1, device=device).float()\n",
    "        xx, yy = torch.meshgrid(ax, ax)\n",
    "        kernel = torch.exp(-(xx**2 + yy**2) / (2 * self.sigma**2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        kernel = kernel.view(1, 1, self.kernel_size, self.kernel_size).repeat(channels, 1, 1, 1)\n",
    "\n",
    "        # Apply Gaussian pooling\n",
    "        x = F.conv2d(x, kernel, padding=self.padding, groups=channels)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class GatedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Multi-Head Attention from \"Gated Attention for Large Language Models: \n",
    "    Non-linearity, Sparsity, and Attention-Sink-Free\" (arXiv:2505.06708)\n",
    "    \n",
    "    핵심 아이디어:\n",
    "    - SDPA (Scaled Dot-Product Attention) 출력 후에 sigmoid gate를 적용\n",
    "    - 수식: Y' = Y ⊙ σ(XW_θ)\n",
    "    - head-specific한 gating으로 attention sink 현상 완화\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        gate_type: str = \"elementwise\",  # \"elementwise\" or \"headwise\"\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 모델의 hidden dimension\n",
    "            num_heads: attention head 수\n",
    "            dropout: dropout 비율\n",
    "            gate_type: \"elementwise\" (각 원소별) 또는 \"headwise\" (head별 단일 게이트)\n",
    "            bias: linear layer에 bias 사용 여부\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.gate_type = gate_type\n",
    "        \n",
    "        # Q, K, V projection layers\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=bias)\n",
    "        \n",
    "        # Gate projection (핵심 추가 부분)\n",
    "        # elementwise: 각 차원마다 독립적인 gate\n",
    "        # headwise: 각 head마다 하나의 gate 값\n",
    "        if gate_type == \"elementwise\":\n",
    "            self.W_gate = nn.Linear(d_model, d_model, bias=bias)\n",
    "        elif gate_type == \"headwise\":\n",
    "            self.W_gate = nn.Linear(d_model, num_heads, bias=bias)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown gate_type: {gate_type}\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "        return_gate_scores: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Attention mask (batch_size, seq_len, seq_len) or (batch_size, 1, seq_len, seq_len)\n",
    "            return_gate_scores: gate 값도 반환할지 여부\n",
    "            \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_model)\n",
    "            gate_scores (optional): gate 값들\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 1. Linear projections for Q, K, V\n",
    "        Q = self.W_q(x)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # 2. Split into multiple heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 3. Scaled Dot-Product Attention (SDPA)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)  # Add head dimension\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Attention output\n",
    "        Y = torch.matmul(attn_weights, V)\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 4. 핵심: Gating mechanism (논문의 G1 position)\n",
    "        # Compute gate scores: σ(XW_θ)\n",
    "        gate_logits = self.W_gate(x)  # (batch_size, seq_len, d_model or num_heads)\n",
    "        gate_scores = torch.sigmoid(gate_logits)\n",
    "        \n",
    "        if self.gate_type == \"elementwise\":\n",
    "            # Element-wise gating: 각 차원마다 독립적인 gate\n",
    "            gate_scores = gate_scores.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "            gate_scores = gate_scores.transpose(1, 2)\n",
    "            # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            \n",
    "            # Apply gate: Y' = Y ⊙ σ(XW_θ)\n",
    "            Y_gated = Y * gate_scores\n",
    "            \n",
    "        elif self.gate_type == \"headwise\":\n",
    "            # Head-wise gating: 각 head마다 하나의 gate 값\n",
    "            gate_scores = gate_scores.view(batch_size, seq_len, self.num_heads, 1)\n",
    "            gate_scores = gate_scores.transpose(1, 2)\n",
    "            # Shape: (batch_size, num_heads, seq_len, 1)\n",
    "            \n",
    "            # Apply gate\n",
    "            Y_gated = Y * gate_scores\n",
    "        \n",
    "        # 5. Concatenate heads\n",
    "        Y_gated = Y_gated.transpose(1, 2).contiguous()\n",
    "        # Shape: (batch_size, seq_len, num_heads, d_k)\n",
    "        Y_gated = Y_gated.view(batch_size, seq_len, d_model)\n",
    "        # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 6. Final output projection\n",
    "        output = self.W_o(Y_gated)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        if return_gate_scores:\n",
    "            return output, gate_scores\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaSpace(nn.Moduule):\n",
    "    def __init__(self, size, feature_dim: int, num_meta_spaces: int):\n",
    "        super().__init__()\n",
    "        self.original_size = size\n",
    "        self.pool = GaussianPooling(kernel_size=5, sigma=2.0)\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_meta_spaces = num_meta_spaces\n",
    "        self.meta_spaces = nn.Parameter(torch.randn(num_meta_spaces, feature_dim))\n",
    "        self.gmha = nn.ModuleList(GatedMultiHeadAttention(\n",
    "            d_model=feature_dim,\n",
    "            num_heads=8,\n",
    "            gate_type=\"headwise\"\n",
    "        ) for _ in range(3))\n",
    "        \n",
    "    def forward_features(self, x: torch.Tensor, keypoints: torch.Tensor) -> torch.Tensor:\n",
    "        pooled_features = []\n",
    "        for feature_space in x[:-1]:\n",
    "            for kpts in keypoints:\n",
    "                resized_kpts = self.cal_resized_keypoints(kpts, feature_space.shape[-2:])\n",
    "                px, py = resized_kpts[:, 0], resized_kpts[:, 1]\n",
    "                patch = feature_space[:, :, py.long(), px.long()]\n",
    "                gaussian_feature = self.pool(patch)\n",
    "\n",
    "                # feature_space와 resized_kpts를 이용한 추가적인 처리 로직 필요\n",
    "            pooled_features.append(self.pool(feature_space))\n",
    "            # 추가적인 처리 로직 필요\n",
    "\n",
    "    def cal_resized_keypoints(self, keypoints: torch.Tensor, target_size: Tuple[int, int]) -> torch.Tensor:\n",
    "        orig_h, orig_w = self.original_size\n",
    "        target_h, target_w = target_size\n",
    "        scale_x = target_w / orig_w\n",
    "        scale_y = target_h / orig_h\n",
    "        resized_keypoints = keypoints.clone()\n",
    "        resized_keypoints[..., 0] *= scale_x\n",
    "        resized_keypoints[..., 1] *= scale_y\n",
    "        return resized_keypoints\n",
    "\n",
    "    def forward(self, x: torch.Tensor, keypoints: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        pass \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSKD(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int\n",
    "        ):\n",
    "        super(FSKD, self).__init__()\n",
    "        self.backbone = torch.hub.load(\"models/backbone/dinov3\", \n",
    "                                       'dinov3_convnext_small', \n",
    "                                       source='local', \n",
    "                                       weights='./checkpoints/dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth')\n",
    "        \n",
    "        self.neck = nn.Sequential()\n",
    "        self.head = nn.Sequential()\n",
    "\n",
    "    def forward_features(\n",
    "            self, \n",
    "            x: torch.Tensor, \n",
    "            masks: Optional[torch.Tensor] = None\n",
    "        ) -> List[Dict[str, torch.Tensor]]:\n",
    "        scaled_features = self.backbone.forward_features_list([x], [masks])[1:]\n",
    "        pose_feature = self.neck(scaled_features)\n",
    "        result = self.head(pose_feature)\n",
    "        return result\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> List[Dict[str, torch.Tensor]]:\n",
    "        result = self.forward_features(x)\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
