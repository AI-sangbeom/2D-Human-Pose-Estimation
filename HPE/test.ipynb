{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dbd828",
   "metadata": {},
   "source": [
    "# Dinov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2664fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from models.backbones import Dinov3ConvNext, Dinov3ViT, convnext_sizes, vit_sizes\n",
    "\n",
    "convnext_ckps = {\n",
    "    'tiny': './checkpoints/dinov3/convnext/dinov3_convnext_tiny_pretrain_lvd1689m-21b726bb.pth', \n",
    "    'small': './checkpoints/dinov3/convnext/dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth', \n",
    "    'base': './checkpoints/dinov3/convnext/dinov3_convnext_base_pretrain_lvd1689m-801f2ba9.pth', \n",
    "    'large': './checkpoints/dinov3/convnext/dinov3_convnext_large_pretrain_lvd1689m-61fa432d.pth'}\n",
    "\n",
    "vit_ckps = {\n",
    "    'small': './checkpoints/dinov3/vit/dinov3_vits16_pretrain_lvd1689m-08c60483.pth', \n",
    "    'base': './checkpoints/dinov3/vit/dinov3_vit7b16_pretrain_lvd1689m-a955f4ea.pth', \n",
    "    'large': None}\n",
    "\n",
    "model_size = 'small'\n",
    "\n",
    "model = Dinov3ConvNext(\n",
    "    depths=convnext_sizes[model_size][\"depths\"],\n",
    "    dims=convnext_sizes[model_size][\"dims\"],    \n",
    "    weights=convnext_ckps[model_size],\n",
    "    )\n",
    "inputs = torch.randn(3, 3, 640, 640)\n",
    "feature_list = model.forward_features([inputs], [None])\n",
    "output = model(inputs)\n",
    "print(output.shape)\n",
    "feature_list['x_norm_patchtokens'].shape\n",
    "\n",
    "model = Dinov3ViT(\n",
    "    patch_size=vit_sizes[model_size][\"patch_size\"],\n",
    "    embed_dim=vit_sizes[model_size][\"embed_dim\"],\n",
    "    depth=vit_sizes[model_size][\"depth\"],\n",
    "    num_heads=vit_sizes[model_size][\"num_heads\"],\n",
    "    ffn_ratio=vit_sizes[model_size][\"ffn_ratio\"],\n",
    "    weights=vit_ckps[model_size],\n",
    "    )\n",
    "inputs = torch.randn(3, 3, 640, 640)\n",
    "feature_list = model.forward_features_list([inputs], [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af7294a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1601, 384])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list[1][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211196c5",
   "metadata": {},
   "source": [
    "# Meta Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MetaSpace with Keypoint Features - Example\n",
      "============================================================\n",
      "\n",
      "1. Initial forward pass (training mode):\n",
      "   Level 0: torch.Size([4, 17, 256])\n",
      "   Level 1: torch.Size([4, 17, 512])\n",
      "   Level 2: torch.Size([4, 17, 1024])\n",
      "\n",
      "2. Update meta spaces:\n",
      "   Meta spaces updated with accumulated features\n",
      "\n",
      "3. Check meta space statistics:\n",
      "   Level 0 meta space: torch.Size([17, 256])\n",
      "      Mean: 0.0005, Std: 0.0774\n",
      "   Level 1 meta space: torch.Size([17, 512])\n",
      "      Mean: -0.0006, Std: 0.0565\n",
      "   Level 2 meta space: torch.Size([17, 1024])\n",
      "      Mean: -0.0000, Std: 0.0411\n",
      "\n",
      "4. Inference mode:\n",
      "   Features fused without accumulation\n",
      "\n",
      "============================================================\n",
      "주요 기능:\n",
      "- Multi-scale keypoint feature extraction\n",
      "- Gaussian pooling for robust local features\n",
      "- EMA-based meta feature learning\n",
      "- Gated attention fusion\n",
      "- Valid mask support for occluded keypoints\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from models.backbones import Dinov3ViT, vit_sizes\n",
    "from models.modules import MetaSpace\n",
    "\n",
    "vit_ckps = {\n",
    "    'small': './checkpoints/dinov3/vit/dinov3_vits16_pretrain_lvd1689m-08c60483.pth', \n",
    "    'base': './checkpoints/dinov3/vit/dinov3_vit7b16_pretrain_lvd1689m-a955f4ea.pth', \n",
    "    'large': None}\n",
    "\n",
    "model_size = 'small'\n",
    "\n",
    "# ===== Usage Example =====\n",
    "model = Dinov3ViT(\n",
    "    patch_size=vit_sizes[model_size][\"patch_size\"],\n",
    "    embed_dim=vit_sizes[model_size][\"embed_dim\"],\n",
    "    depth=vit_sizes[model_size][\"depth\"],\n",
    "    num_heads=vit_sizes[model_size][\"num_heads\"],\n",
    "    ffn_ratio=vit_sizes[model_size][\"ffn_ratio\"],\n",
    "    weights=vit_ckps[model_size],\n",
    "    )\n",
    "print(\"=\" * 60)\n",
    "print(\"MetaSpace with Keypoint Features - Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "num_kpts = 17  # e.g., human pose keypoints\n",
    "original_size = (256, 256)\n",
    "\n",
    "# Multi-scale feature maps (e.g., from backbone)\n",
    "feature_maps = [\n",
    "    torch.randn(batch_size, 256, 64, 64),   # Level 0: 1/4 scale\n",
    "    torch.randn(batch_size, 512, 32, 32),   # Level 1: 1/8 scale\n",
    "    torch.randn(batch_size, 1024, 16, 16),  # Level 2: 1/16 scale\n",
    "]\n",
    "\n",
    "# Keypoints in original image coordinates\n",
    "keypoints = torch.rand(batch_size, num_kpts, 2) * 256  # Random [0, 256]\n",
    "\n",
    "# Valid mask (e.g., some keypoints are occluded)\n",
    "valid_mask = torch.rand(batch_size, num_kpts) > 0.2\n",
    "\n",
    "# Initialize MetaSpace\n",
    "meta_space = MetaSpace(\n",
    "    original_size=original_size,\n",
    "    feature_dims=[256, 512, 1024],\n",
    "    num_kpts=num_kpts,\n",
    "    num_heads=8,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "print(\"\\n1. Initial forward pass (training mode):\")\n",
    "meta_space.train()\n",
    "fused_features = meta_space(feature_maps, keypoints, valid_mask)\n",
    "\n",
    "for i, feats in enumerate(fused_features):\n",
    "    print(f\"   Level {i}: {feats.shape}\")\n",
    "\n",
    "print(\"\\n2. Update meta spaces:\")\n",
    "meta_space.update_meta_spaces()\n",
    "print(\"   Meta spaces updated with accumulated features\")\n",
    "\n",
    "print(\"\\n3. Check meta space statistics:\")\n",
    "for i, meta in enumerate(meta_space.meta_spaces):\n",
    "    print(f\"   Level {i} meta space: {meta.shape}\")\n",
    "    print(f\"      Mean: {meta.mean().item():.4f}, Std: {meta.std().item():.4f}\")\n",
    "\n",
    "print(\"\\n4. Inference mode:\")\n",
    "meta_space.eval()\n",
    "with torch.no_grad():\n",
    "    fused_features_eval = meta_space(feature_maps, keypoints)\n",
    "print(\"   Features fused without accumulation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"주요 기능:\")\n",
    "print(\"- Multi-scale keypoint feature extraction\")\n",
    "print(\"- Gaussian pooling for robust local features\")\n",
    "print(\"- EMA-based meta feature learning\")\n",
    "print(\"- Gated attention fusion\")\n",
    "print(\"- Valid mask support for occluded keypoints\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90408d",
   "metadata": {},
   "source": [
    "# FSKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSKD(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int\n",
    "        ):\n",
    "        super(FSKD, self).__init__()\n",
    "        self.backbone = torch.hub.load(\"models/backbone/dinov3\", \n",
    "                                       'dinov3_convnext_small', \n",
    "                                       source='local', \n",
    "                                       weights='./checkpoints/dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth')\n",
    "        \n",
    "        self.neck = nn.Sequential()\n",
    "        self.head = nn.Sequential()\n",
    "\n",
    "    def forward_features(\n",
    "            self, \n",
    "            x: torch.Tensor, \n",
    "            masks: Optional[torch.Tensor] = None\n",
    "        ) -> List[Dict[str, torch.Tensor]]:\n",
    "        scaled_features = self.backbone.forward_features_list([x], [masks])[1:]\n",
    "        pose_feature = self.neck(scaled_features)\n",
    "        result = self.head(pose_feature)\n",
    "        return result\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> List[Dict[str, torch.Tensor]]:\n",
    "        result = self.forward_features(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c69aa",
   "metadata": {},
   "source": [
    "# Autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f16f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.amp.autocast_mode.autocast at 0x76d12b671e10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.amp as amp \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "amp.autocast(device, enabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546e771",
   "metadata": {},
   "source": [
    "# DeepPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30024a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import timm.models.resnet as resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73765626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.resnet import Bottleneck\n",
    "from timm.models.resnet import BasicBlock\n",
    "from timm.models.helpers import load_pretrained, resolve_pretrained_cfg\n",
    "\n",
    "resnet_args = {\n",
    "    'resnet18': dict(block=BasicBlock, layers=(2, 2, 2, 2)),\n",
    "    'resnet34': dict(block=BasicBlock, layers=(3, 4, 6, 3)),\n",
    "    'resnet50': dict(block=Bottleneck, layers=(3, 4, 6, 3)),\n",
    "    'resnet50s': dict(block=Bottleneck, layers=(3, 4, 6, 3), stem_width=64, stem_type='deep'),\n",
    "    'resnet50t': dict(block=Bottleneck, layers=(3, 4, 6, 3), stem_width=32, stem_type='deep_tiered', avg_down=True),\n",
    "    'resnet101': dict(block=Bottleneck, layers=(3, 4, 23, 3))\n",
    "}\n",
    "\n",
    "resnet_ckps = {\n",
    "    'resnet18': './checkpoints/resnet/resnet18-5c106cde.pth',\n",
    "    'resnet34': './checkpoints/resnet/resnet34-333f7ec4.pth',\n",
    "    'resnet50': './checkpoints/resnet/resnet50-0676ba61.pth',\n",
    "    'resnet50s': './checkpoints/resnet/resnet50s-3cf99910.pth',\n",
    "    'resnet50t': './checkpoints/resnet/resnet50t-1f8793b8.pth',\n",
    "    'resnet101': './checkpoints/resnet/resnet101-5d3b4d8f.pth'\n",
    "}\n",
    "\n",
    "class DeepPose(timm.models.ResNet):\n",
    "    def __init__(self, num_joints=8, backbone='resnet50', checkpoint=None, download=False, **kwargs):\n",
    "        model_args = resnet_args.get(backbone)\n",
    "        super(DeepPose, self).__init__(**dict(model_args, **kwargs))\n",
    "        features = False\n",
    "\n",
    "        # resolve and update model pretrained config and model kwargs\n",
    "        pretrained_cfg = resolve_pretrained_cfg(\n",
    "            backbone,\n",
    "            pretrained_cfg=None,\n",
    "            pretrained_cfg_overlay=None\n",
    "        )\n",
    "        pretrained_cfg = pretrained_cfg.to_dict()\n",
    "        \n",
    "        # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n",
    "        num_classes_pretrained = 0 if features else getattr(self, 'num_classes', kwargs.get('num_classes', 1000))\n",
    "        if download:\n",
    "            checkpoint = load_pretrained(\n",
    "                timm.models.ResNet,\n",
    "                pretrained_cfg=pretrained_cfg,\n",
    "                num_classes=num_classes_pretrained,\n",
    "                in_chans=kwargs.get('in_chans', 3),\n",
    "                filter_fn=None,\n",
    "                strict=None,\n",
    "                cache_dir=None,\n",
    "                )\n",
    "        print(checkpoint)\n",
    "        if checkpoint:\n",
    "            self.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "        self.njoints = num_joints\n",
    "        self.fc = nn.Linear(self.fc.in_features, self.njoints*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f08b51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.load_state_dict() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDeepPose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m random_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[51], line 40\u001b[0m, in \u001b[0;36mDeepPose.__init__\u001b[0;34m(self, num_joints, backbone, checkpoint, download, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m num_classes_pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 40\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResNet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_chans\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(checkpoint)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint:\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/hpe/lib/python3.10/site-packages/timm/models/_builder.py:280\u001b[0m, in \u001b[0;36mload_pretrained\u001b[0;34m(model, pretrained_cfg, num_classes, in_chans, filter_fn, strict, cache_dir)\u001b[0m\n\u001b[1;32m    277\u001b[0m             classifier_bias \u001b[38;5;241m=\u001b[39m state_dict[classifier_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    278\u001b[0m             state_dict[classifier_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m classifier_bias[label_offset:]\n\u001b[0;32m--> 280\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_result\u001b[38;5;241m.\u001b[39mmissing_keys:\n\u001b[1;32m    282\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing keys (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(load_result\u001b[38;5;241m.\u001b[39mmissing_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) discovered while loading pretrained weights.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m This is expected if model is being adapted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.load_state_dict() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    import torch \n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = DeepPose(8, 'resnet18', download=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    random_noise = torch.randn((32, 3, 224, 224)).to(device)\n",
    "    result = model(random_noise)\n",
    "\n",
    "    print(f'INPUT SIZE : {random_noise.shape}')\n",
    "    print(f'OUTPUT SIZE : {result.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc21e1",
   "metadata": {},
   "source": [
    "# FCMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda7fba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MinkowskiConvolution' from 'MinkowskiEngine' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mthirdparty\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mConvNeXt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FCMAE\n",
      "File \u001b[0;32m~/study/2D-Human-Pose-Estimation/HPE/thirdparty/ConvNeXt/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m \n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfcmae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FCMAE\n",
      "File \u001b[0;32m~/study/2D-Human-Pose-Estimation/HPE/thirdparty/ConvNeXt/models/fcmae.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMinkowskiEngine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     MinkowskiConvolution,\n\u001b[1;32m     14\u001b[0m     MinkowskiDepthwiseConvolution,\n\u001b[1;32m     15\u001b[0m     MinkowskiLinear,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trunc_normal_\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnextv2_sparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseConvNeXtV2\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MinkowskiConvolution' from 'MinkowskiEngine' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), \"thirdparty\", \"ConvNeXt\"))\n",
    "from thirdparty.ConvNeXt import FCMAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
